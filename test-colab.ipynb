{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9qEGTK3dHzn"
      },
      "source": [
        "Testing google colab and github wooo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "NCVLA88pdHzu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just going to run a mock transformer model I built"
      ],
      "metadata": {
        "id": "wibPtdCGfZ8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dmodel = 512    # embedding, internal latent vector size\n",
        "h = 8           # number of vkq vectors to spawn for attention\n",
        "dk = int(dmodel / h) # size of vkq vectors for concatenation\n",
        "inSeqLen = 250  # input sequence length\n",
        "dff = 2048       # feed forward internal length"
      ],
      "metadata": {
        "id": "eYfTuVu8ew7C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = None\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "d7GafN1NewJl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6tpSYqMudHzx"
      },
      "outputs": [],
      "source": [
        "class SDAttention(torch.nn.Module):\n",
        "  def __init__(self, masked=False):\n",
        "    super(SDAttention, self).__init__()\n",
        "    self.smax = torch.nn.Softmax(dim=-1)\n",
        "    self.masked = masked\n",
        "\n",
        "  def forward(self, v, k, q):\n",
        "    self_attIn = torch.matmul(q, torch.transpose(k, -1, -2))\n",
        "    if self.masked:\n",
        "      self_attIn[self_attIn == 0] = -torch.inf\n",
        "    self_attention = self.smax(self_attIn / (dk ** 0.5))\n",
        "    return torch.matmul(self_attention, v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHead(torch.nn.Module):\n",
        "  def __init__(self, dmodel, dk, h, masked=False):\n",
        "    super().__init__()\n",
        "    self.sdattention = SDAttention(masked=masked)\n",
        "    self.vmats = [torch.zeros([dmodel, dk], requires_grad=True, device=device) for _ in range(h)]\n",
        "    self.kmats = [torch.zeros([dmodel, dk], requires_grad=True, device=device) for _ in range(h)]\n",
        "    self.qmats = [torch.zeros([dmodel, dk], requires_grad=True, device=device) for _ in range(h)]\n",
        "    self.outMat = torch.zeros([dmodel, dmodel], requires_grad=True, device=device)\n",
        "    for vmat in self.vmats:\n",
        "      torch.nn.init.xavier_normal_(vmat)\n",
        "    for kmat in self.kmats:\n",
        "      torch.nn.init.xavier_normal_(kmat)\n",
        "    for qmat in self.qmats:\n",
        "      torch.nn.init.xavier_normal_(qmat)    \n",
        "    torch.nn.init.xavier_normal_(self.outMat)\n",
        "\n",
        "    self.params = self.vmats + self.kmats + self.qmats\n",
        "    self.params.append(self.outMat)\n",
        "\n",
        "  def forward(self, v, k, q):\n",
        "    outs = []\n",
        "    for i in range(h):\n",
        "      vmat = self.vmats[i]\n",
        "      kmat = self.kmats[i]\n",
        "      qmat = self.qmats[i]\n",
        "\n",
        "      vIn = torch.matmul(v, vmat)\n",
        "      kIn = torch.matmul(k, kmat)\n",
        "      qIn = torch.matmul(q, qmat)\n",
        "\n",
        "      attOut = self.sdattention(vIn, kIn, qIn)\n",
        "\n",
        "      outs.append(attOut)\n",
        "    out = torch.concat(outs, dim=-1)\n",
        "    out = torch.matmul(out, self.outMat)\n",
        "    return out"
      ],
      "metadata": {
        "id": "HchzLoXsermg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selfAtt = MultiHead(dmodel, dk, h, True)\n",
        "v = torch.rand([inSeqLen, dmodel], device=device)\n",
        "k = torch.rand([inSeqLen, dmodel], device=device)\n",
        "q = torch.rand([inSeqLen, dmodel], device=device)\n",
        "selfAtt(v,k,q).shape"
      ],
      "metadata": {
        "id": "v8R0lOIle2bG",
        "outputId": "a9c6c0cd-540c-4448-d0ad-200754dadfbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([250, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionEncode(torch.nn.Module):\n",
        "  def __init__(self, seq_len, dmodel):\n",
        "    super().__init__()\n",
        "    encoded = torch.zeros([seq_len, dmodel])\n",
        "    for pos in range(seq_len):\n",
        "      for i in range(0, dmodel, 2):\n",
        "        encoded[pos][i] = math.sin(pos / (10000 ** (i / dmodel)))\n",
        "        encoded[pos][i+1] = math.cos(pos / (10000 ** (i / dmodel)))\n",
        "    self.encoded = encoded.to(device)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    encoded = self.encoded\n",
        "    #if len(x.shape) == 3:\n",
        "    #  encoded = torch.stack([encoded for _ in range(x.shape[0])])\n",
        "    \n",
        "    return x + encoded"
      ],
      "metadata": {
        "id": "q3z1mGsue79x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FF(torch.nn.Module):\n",
        "  def __init__(self, dmodel, dff):\n",
        "    super().__init__()\n",
        "    self.lin0 = torch.nn.Linear(dmodel, dff, device=device)\n",
        "    #torch.nn.init.kaiming_normal_(self.lin0.weight)\n",
        "    torch.nn.init.xavier_normal_(self.lin0.weight)\n",
        "    self.lin1 = torch.nn.Linear(dff, dmodel, device=device)\n",
        "    torch.nn.init.xavier_normal_(self.lin1.weight)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.dropout = torch.nn.Dropout(p=0.1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.lin0(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.lin1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "yC1hhvXge_OY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(torch.nn.Module):\n",
        "  def __init__(self, seq_len, dmodel, dk, h, dff):\n",
        "    super().__init__()\n",
        "    self.multiHead = MultiHead(dmodel, dk, h)\n",
        "    self.ff = FF(dmodel, dff)\n",
        "    self.layerNorm0 = torch.nn.LayerNorm(dmodel, device=device)\n",
        "    self.layerNorm1 = torch.nn.LayerNorm(dmodel, device=device)\n",
        "    self.dropout = torch.nn.Dropout(p=0.1)\n",
        "\n",
        "    self.params = list(self.ff.parameters()) + self.multiHead.params + list(self.layerNorm0.parameters()) + list(self.layerNorm1.parameters())\n",
        "\n",
        "  def forward(self, x):\n",
        "    headOut = self.multiHead(x, x, x)\n",
        "    headOut = self.dropout(headOut)\n",
        "    x = self.layerNorm0(headOut +  x)\n",
        "\n",
        "    feedOut = self.ff(x)\n",
        "    feedOut = self.dropout(feedOut)\n",
        "    x = self.layerNorm1(feedOut + x)\n",
        "    \n",
        "    return x"
      ],
      "metadata": {
        "id": "foO7zXYcfFwT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inVal = torch.rand([3, inSeqLen, dmodel], device=device)\n",
        "encoder = TransformerEncoder(inSeqLen, dmodel, dk, h, dff).to(device)\n",
        "encoder(inVal).shape"
      ],
      "metadata": {
        "id": "QwVhoeDLfMCK",
        "outputId": "4154a697-9120-40ea-b958-fb8140fd89d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 250, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Senti(torch.nn.Module):\n",
        "  def __init__(self, seq_len, dmodel, dk, h, dff):\n",
        "    super().__init__()\n",
        "    self.posEncode = PositionEncode(seq_len, dmodel)\n",
        "    encoders = [TransformerEncoder(seq_len, dmodel, dk, h, dff) for _ in range(3)]\n",
        "    self.encoders = torch.nn.Sequential(*encoders)\n",
        "    self.flatten = torch.nn.Flatten()\n",
        "\n",
        "    lin0 = torch.nn.Linear(seq_len * dmodel, 1)\n",
        "    torch.nn.init.xavier_normal_(lin0.weight)\n",
        "    #lin1 = torch.nn.Linear(1000, 100)\n",
        "    #torch.nn.init.xavier_normal_(lin1.weight)\n",
        "    #lin2 = torch.nn.Linear(100, 1)\n",
        "    #torch.nn.init.xavier_normal_(lin2.weight)\n",
        "    self.ff = torch.nn.Sequential(\n",
        "                                  #torch.nn.BatchNorm1d(seq_len * dmodel),\n",
        "                                  lin0,\n",
        "                                  #torch.nn.ReLU(),\n",
        "                                  #torch.nn.BatchNorm1d(1000),\n",
        "                                  #lin1,\n",
        "                                  #torch.nn.ReLU(),\n",
        "                                  #torch.nn.BatchNorm1d(100),\n",
        "                                  #lin2,\n",
        "                                  torch.nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "    encoderParams = []\n",
        "    for encoder in self.encoders:\n",
        "      encoderParams += encoder.params\n",
        "    self.params = encoderParams + list(self.ff.parameters())\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.posEncode(x)\n",
        "    x = self.encoders(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.ff(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "zYunFjwSfO7w"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inVal = torch.rand([3, inSeqLen, dmodel], device=device)\n",
        "senti = Senti(inSeqLen, dmodel, dk, h, dff).to(device)\n",
        "senti(inVal).shape"
      ],
      "metadata": {
        "id": "BD-SFhMAfTCd",
        "outputId": "8637c037-129e-405c-e051-9da666e18b6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "test-colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}